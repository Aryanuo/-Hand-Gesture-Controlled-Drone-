# âœ‹ Hand Gesture Controlled Drone (MediaPipe + MLP)

This project implements a **real-time hand gestureâ€“based control system** using a webcam.  
Hand gestures are detected live and converted into **drone movement commands** such as takeoff, land, left, right, up, down, forward, and backward.

The project combines **Computer Vision + Machine Learning** using:
- **OpenCV** for camera input
- **MediaPipe** for hand landmark detection
- **MLP (Multi-Layer Perceptron)** for gesture classification

> âš ï¸ Currently, drone commands are implemented as **dummy functions (print statements)**.  
> The code structure is ready for integration with a real drone SDK (e.g., DJI Tello).

---

## ğŸ§  How the System Works (Simple Overview)

Webcam
â†“
OpenCV captures video frames
â†“
MediaPipe detects hand & 21 landmarks
â†“
MLP model classifies the gesture
â†“
Gesture mapped to drone command


Instead of working on raw image pixels, the system uses **hand landmarks**, which makes it:
- Robust to background changes
- Less sensitive to lighting
- Easier and faster to train

---

## âœ‹ Supported Gestures & Commands

| Gesture | Command |
|------|------|
| âœ‹ Open palm | Takeoff |
| âœŠ Closed fist | Land |
| ğŸ‘ˆ Thumb left | Move Left |
| ğŸ‘‰ Thumb right | Move Right |
| ğŸ‘ Thumb up | Move Up |
| ğŸ‘ Thumb down | Move Down |
|     Index finger upward| Move Forward |
|     Two finger upward| Move Backward |

ğŸ‘‰ Thumb-only gestures are used for motion commands(up,down,left,right) to reduce confusion.

---

## ğŸ“ Project Structure

hand_gesture_drone/
â”‚
â”œâ”€â”€ camera.py # Opens the webcam using OpenCV
â”œâ”€â”€ hand_tracker.py # MediaPipe hand landmark detection
â”œâ”€â”€ data_collector.py # Collects labeled training data
â”œâ”€â”€ train_mlp.py # Trains the MLP classifier
â”œâ”€â”€ predict.py # Loads model and predicts gestures
â”œâ”€â”€ drone_commands.py # Dummy drone command functions
â”œâ”€â”€ main.py # Runs the live gesture control system
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ gestures.csv # Collected gesture dataset
â”‚
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ gesture_mlp.pkl # Trained MLP model
â”‚ â””â”€â”€ label_encoder.pkl # Label encoder
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ› ï¸ Requirements

- Python **3.10**
- A working webcam
- Virtual environment (recommended)

### Required Python Libraries

All dependencies are listed in `requirements.txt`:
- opencv-python
- mediapipe
- numpy
- pandas
- scikit-learn
- joblib

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/your-username/hand-gesture-drone.git
cd hand-gesture-drone

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt
ğŸ“Š Step 1: Collect Gesture Data
Run the data collection script:

python data_collector.py
How it works:

Enter a gesture label (e.g., LEFT)

Hold the gesture in front of the camera for 3â€“5 seconds

Multiple samples are automatically recorded (one per frame)

Press q to stop

Repeat for all gestures

The data is saved to:

data/gestures.csv
ğŸ§  Step 2: Train the Model
Train the MLP gesture classifier:

python train_mlp.py
This step:

Reads gestures.csv

Trains an MLP neural network

Prints evaluation metrics:

Accuracy

Confusion Matrix

Precision, Recall, F1-score

Saves trained files to:

models/
ğŸ” Re-run this step only when the dataset changes.

â–¶ï¸ Step 3: Run the Live Gesture Control System
Start real-time gesture recognition:

python main.py
What happens:

Webcam window opens

Hand landmarks are drawn

Predicted gesture is displayed

Corresponding drone command is printed

Press q or ESC to exit


ğŸ§ª Typical Workflow
First-time setup:
data_collector.py â†’ train_mlp.py â†’ main.py
Adding new gestures later:
data_collector.py â†’ train_mlp.py â†’ main.py
Daily demo / testing:
main.py only
